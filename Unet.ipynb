{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyODrIAsle/PIGZi7XYuV3uH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Salva13s/HybridModel/blob/main/Unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg36EzW6Bdwt",
        "outputId": "7da31f51-c3b0-4e4f-9bda-2bb64d9d28fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.metrics import r2_score\n",
        "from torchvision.utils import save_image"
      ],
      "metadata": {
        "id": "q956q5NQCGCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(30)"
      ],
      "metadata": {
        "id": "Ld3ESCfvCLGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FruitDataset(Dataset):\n",
        "    def __init__(self, image_dir, density_map_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.density_map_dir = density_map_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(('.jpg'))])\n",
        "        self.density_map_files = sorted([f for f in os.listdir(density_map_dir) if f.endswith(('.jpg'))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        density_map_path = os.path.join(self.density_map_dir, self.density_map_files[idx])\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        density_map = Image.open(density_map_path).convert('L')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        density_map_np = np.array(density_map)\n",
        "        density_map_np = cv2.GaussianBlur(density_map_np, (5, 5), sigmaX=1.0)\n",
        "        density_map_np = density_map_np / np.max(density_map_np) if np.max(density_map_np) > 0 else density_map_np\n",
        "\n",
        "        density_map = torch.tensor(density_map_np, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        return image, density_map"
      ],
      "metadata": {
        "id": "ogdT91hPCOH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def conv_block(in_channels, out_channels):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        def upconv_block(in_channels, out_channels):\n",
        "            return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "        self.encoder1 = conv_block(in_channels, 64)\n",
        "        self.encoder2 = conv_block(64, 128)\n",
        "        self.encoder3 = conv_block(128, 256)\n",
        "        self.encoder4 = conv_block(256, 512)\n",
        "        self.encoder5 = conv_block(512, 1024)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.upconv4 = upconv_block(1024, 512)\n",
        "        self.decoder4 = conv_block(1024, 512)\n",
        "        self.upconv3 = upconv_block(512, 256)\n",
        "        self.decoder3 = conv_block(512, 256)\n",
        "        self.upconv2 = upconv_block(256, 128)\n",
        "        self.decoder2 = conv_block(256, 128)\n",
        "        self.upconv1 = upconv_block(128, 64)\n",
        "        self.decoder1 = conv_block(128, 64)\n",
        "\n",
        "        self.conv_final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool(enc1))\n",
        "        enc3 = self.encoder3(self.pool(enc2))\n",
        "        enc4 = self.encoder4(self.pool(enc3))\n",
        "        enc5 = self.encoder5(self.pool(enc4))\n",
        "\n",
        "        dec4 = self.upconv4(enc5)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "\n",
        "        return self.conv_final(dec1)"
      ],
      "metadata": {
        "id": "fdgc4S0MCVWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(unet, train_loader, val_loader, num_epochs=50):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(unet.parameters(), lr=1e-4)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    unet.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        unet.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, density_maps in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            density_maps = density_maps.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = unet(inputs)\n",
        "            loss = criterion(outputs, density_maps)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}, Training Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        validate_model(unet, val_loader)\n",
        "\n",
        "# Funci√≥n para validar el modelo\n",
        "def validate_model(unet, val_loader):\n",
        "    unet.eval()\n",
        "    running_loss = 0.0\n",
        "    criterion = nn.MSELoss()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, density_maps in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            density_maps = density_maps.to(device)\n",
        "\n",
        "            outputs = unet(inputs)\n",
        "            loss = criterion(outputs, density_maps)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    val_loss = running_loss / len(val_loader.dataset)\n",
        "    print(f'Validation Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "id": "x_G4oen1CcLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los datos\n",
        "train_loader = DataLoader(\n",
        "    FruitDataset(image_dir='/content/drive/MyDrive/1 Tesis/Fish/DataDividido/train/reales_resized',\n",
        "                 density_map_dir='/content/drive/MyDrive/1 Tesis/Fish/DataDividido/train/generadas_resized',\n",
        "                 transform=transforms.ToTensor()),\n",
        "    batch_size=6, shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    FruitDataset(image_dir='/content/drive/MyDrive/1 Tesis/Fish/DataDividido/val/reales_resized',\n",
        "                 density_map_dir='/content/drive/MyDrive/1 Tesis/Fish/DataDividido/val/generadas_resized',\n",
        "                 transform=transforms.ToTensor()),\n",
        "    batch_size=6, shuffle=False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    FruitDataset(image_dir='/content/drive/MyDrive/1 Tesis/Fish/DataDividido/test/reales_resized',\n",
        "                 density_map_dir='/content/drive/MyDrive/1 Tesis/Fish/DataDividido/test/generadas_resized',\n",
        "                 transform=transforms.ToTensor()),\n",
        "    batch_size=6, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "dV9ejKFyCfdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet = UNet()\n",
        "train_model(unet, train_loader, val_loader, num_epochs=50)"
      ],
      "metadata": {
        "id": "k6pUX3opDfHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import save_image\n",
        "\n",
        "def evaluate_and_save_predictions(unet, test_loader, save_dir):\n",
        "    unet.eval()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, true_density_maps) in enumerate(test_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            true_density_maps = true_density_maps.to(device)\n",
        "\n",
        "            predicted_density_maps = unet(inputs)\n",
        "\n",
        "            image_filenames = test_loader.dataset.image_files[batch_idx * test_loader.batch_size: (batch_idx + 1) * test_loader.batch_size]\n",
        "\n",
        "            for i in range(len(predicted_density_maps)):\n",
        "                pred_map = predicted_density_maps[i].cpu().squeeze(0)\n",
        "\n",
        "                real_filename = image_filenames[i]\n",
        "                pred_filename = f'{real_filename}'\n",
        "\n",
        "                save_image(pred_map, os.path.join(save_dir, pred_filename))\n",
        "\n",
        "    print(f'Predictions saved to {save_dir}')\n",
        "\n",
        "save_directory = '/content/drive/MyDrive/1 Tesis/Fish/Unet/fishPred50'\n",
        "evaluate_and_save_predictions(unet, test_loader, save_directory)\n"
      ],
      "metadata": {
        "id": "ZpydV2spdkBM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}